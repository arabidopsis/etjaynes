<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
    <meta name="Author" content="Ian Castleden" />
    <title>E.T. Jaynes</title>
    <style>
      body {
        max-width: 50vw;
        margin: 1em;
        margin-left: 25vw;
        border: solid #eeeeee 1px;
        padding: 1em;
        font-family: system-ui, -apple-system, "Segoe UI", Roboto,
          "Helvetica Neue", Arial, "Noto Sans", "Liberation Sans", sans-serif;
      }
      figure,
      figcaption {
        font-family: monospace;
      }
    </style>
  </head>

  <body>
    <h1>Probability Theory: The Logic of Science</h1>
    <ol>
      <li>
        <a href="jaynes/cpreambl-index-preface.pdf"><b>Preface</b></a
        >. Title/Index/Preface
      </li>
    </ol>
    <hr />
    <ol>
      <li>
        <a href="jaynes/cc01p-plausible-reasoning.pdf"
          ><b>Plausible Reasoning</b></a
        >. Propositional Logic and Boolean Algebra.
      </li>
      <li>
        <a href="jaynes/cc02m-the-quantitative-rules.pdf"
          ><b>The Quantitative Rules</b></a
        >. Cox's Rules.
      </li>
      <li>
        <a href="jaynes/cc03o-elementary-sampling-theory.pdf"
          ><b>Elementary Sampling Theory</b></a
        >. Sampling Distributions make predictions, [...], about potential
        observations.

        <figure>
          <a href="jaynes/cfig3-1.pdf">Figure 3.1</a>:
          <figcaption>
            The Hypergeometric Distribution for N = 15, 30, 100, &infin;
          </figcaption>
        </figure>
      </li>
      <li>
        <a href="jaynes/cc04q-elementary-hypothesis-testing.pdf"
          ><b>Elementary Hypothesis Testing</b></a
        >.
        <figure>
          <a href="jaynes/cfig4-1.pdf">Figure 4.1</a>:
          <figcaption>
            A Surprising Multiple Sequential Test Wherein a Dead Hypothesis (C)
            is Resurrected.
          </figcaption>
        </figure>
      </li>
      <li>
        <a href="jaynes/cc05e-queer-uses-for-probability-theory.pdf"
          ><b>Queer Uses For Probability Theory</b></a
        >.
        <blockquote>
          Kahneman &amp; Tversky claimed that we are not Bayesians, because in
          psychological tests people often commint violations of Bayesian
          principles. However, this claim is seen differently in vliew of what
          we have just noted. We suggest that people are reasoning according to
          a more sophisticated version of Bayesian inference than they had in
          mind.
        </blockquote>
      </li>
      <li>
        <a href="jaynes/cc06q-elmentary-parameter-estimation.pdf"
          ><b>Elementary Parameter Estimation</b></a
        >. Pulling balls from Urns... See also
        <a href="jaynes/articles/cmonkeys.pdf">here</a>.
        <figure>
          <a href="jaynes/cfig6-1.pdf">Figure 6.1</a>:
          <figcaption>The Causal Influences.</figcaption>
        </figure>
        <figure>
          <a href="jaynes/cfig6-2.pdf">Figure 6.2</a>:
          <figcaption>The Strucutre of Mr A's Problem....</figcaption>
        </figure>
      </li>
      <li>
        <a href="jaynes/cc07s-the-central-gaussian-or-normal-distribution.pdf"
          ><b>The Central Gaussian, Or Normal, Distribution</b></a
        >. Galton's regression to the mean discovery and other things Gaussian.
      </li>
      <li>
        <a href="jaynes/cc08n-sufficiency-ancillarity-and-all-that.pdf"
          ><b>Sufficiency, Ancillarity, And All That</b></a
        >. Also Combining Evidence from Different Sources (meta-analysis).
      </li>
      <li>
        <a
          href="jaynes/cc09i-repeditive-experiments-probability-and-frequency.pdf"
          ><b>Repetitive Experiments &mdash; Probability and Frequency</b></a
        >.
      </li>
      <li>
        <a href="jaynes/cc10k-physcis-of-random-experiments.pdf"
          ><b>Physics of &ldquo;Random Experiments&rdquo;</b></a
        >. Heads and Tails in Coin "Phase Space".
      </li>
      <li>
        <a
          href="jaynes/cc11g-discrete-prior-probabilities-the-entropy-principle.pdf"
          ><b>Discrete Prior Probabilities &mdash; The Entropy Principle</b></a
        >. The Principle of Maximum Entropy.
      </li>
      <li><b>Ignorance Priors and Transformation Groups</b>. (No text)</li>
      <li>
        <a href="jaynes/cc13v-decision-theory-historical-background.pdf"
          ><b>Decision Theory &mdash; Historical Background</b></a
        >. The Rationale of Insurance.
      </li>
      <li>
        <a href="jaynes/cc14g-simple-applications-of-decision-theory.pdf"
          ><b>Simple Applications of Decision Theory</b></a
        >.
      </li>
      <li>
        <a href="jaynes/cc15b-paradoxes-of-probability-theory.pdf"
          ><b>Paradoxes of Probability Theory</b></a
        >. Nonconglomerability. The Borel-Kolmogorov Paradox.
      </li>
      <li>
        <a href="jaynes/cc16v-orthodox-methods-historical-background.pdf"
          ><b>Orthodox Methods: Historical Background</b></a
        >.
        <figure>
          <a href="jaynes/cfig15-1.pdf">Figure 15.1</a>:
          <figcaption>
            Solution to the "Strong Inconsistency" Problem for n=100 Tosses.
          </figcaption>
        </figure>
      </li>
      <li>
        <a
          href="jaynes/cc17h-principles-and-pathologies-of-orthodox-statistics.pdf"
          ><b>Principle and Pathology of Orthodox Statistics</b></a
        >. Unbiased Estimators.
      </li>
      <li>
        <a href="jaynes/cc18i-the-Ap-distribution-and-rule-of-succession.pdf"
          ><b>The A<sub>p</sub> Distribution and Rule of Succession</b></a
        >. Outer and Inner Robots.
      </li>
      <li>
        <a href="jaynes/cc19g-physical-measurements.pdf"
          ><b>Physical Measurments</b></a
        >.
      </li>
      <li>
        <a href="jaynes/cc20b-trend-and-seasonality-in-time-series.pdf"
          ><b>Trends And Seasonality in Time Series</b></a
        >. Fitting Time series a la Bretthorst. (Not in Book)
      </li>
      <li>
        <a href="jaynes/cc21a-regression-and-linear-models.pdf"
          ><b>Regression and Linear Models</b></a
        >. (Not in Book)
      </li>
      <li>
        <a href="jaynes/cc24j-model-comparison.pdf"><b>Model Comparison</b></a
        >. (Chapter 20 in Book)
      </li>
      <li><b>Outliers and Robustness</b> (Chapter 21 in Book)</li>
      <li>
        <a href="jaynes/cc27d-introduction-to-communication-theory.pdf"
          ><b>Introduction to Communication Theory</b></a
        >. Shannon etc. (Chapter 22 in Book)
      </li>
      <li>
        <a href="jaynes/cc30e-maximum-entropy-matrix-formulation.pdf"
          ><b>Maximum Entropy: Matrix Formulation</b></a
        >. (Not in Book)
      </li>
    </ol>
    <h3>References</h3>
    <ol>
      <li>
        <a href="jaynes/crefsv.pdf"><b>References</b></a>
      </li>
    </ol>
    <h3>Appendicies</h3>
    <ol>
      <li>
        <a href="jaynes/cappal-the-kolmogorov-system-of-probabilities.pdf"
          ><b>Appendix A</b></a
        >. Other Approaches to Probability Theory.
      </li>
      <li>
        <a href="jaynes/cappb8-mathematical-formalities-and-style.pdf"
          ><b>Appendix B</b></a
        >. Mathematical Formalities and Styles.
      </li>

      <li>
        <a href="jaynes/cappc1-convolutions-and-cumulants.pdf"
          ><b>Appendix C</b></a
        >. Convolution and Cumulants.
      </li>
      <li>
        <a href="jaynes/cappe1-multivariate-gaussian-integrals.pdf"
          ><b>Appendix E</b></a
        >. Multivariate Gaussian Integrals.
      </li>
    </ol>

    <hr />

    <h2>
      Jaynes' Articles from
      <a href="http://bayes.wustl.edu">http://bayes.wustl.edu/</a>
    </h2>

    <ol>
      <li>
        <a href="jaynes/articles/theory.1.pdf"
          ><b>Information Theory and Statistical Mechanics</b></a
        >. <i>The Physical Review, Vol. 106, No. 4, 620-630</i>. May 15, 1957.
      </li>
      <li>
        <a href="jaynes/articles/confidence.pdf"
          ><b>Confidence Intervals vs Bayesian Intervals</b></a
        >. Harper and Hooker (eds.).
        <i
          >Foundations of Probability Theory, Statistical Inference and
          Statistical Theories of Science, Vol II</i
        >. (1976) 175-257.
      </li>
      <li>
        <a href="jaynes/articles/stand.on.entropy.pdf"
          ><b>Where do we Stand on Maximum Entropy?</b></a
        >. To be presented at the Maximum Entropy Formalism Conference,
        Massachusetts Institute of Technology, May 2-4, 1978.
      </li>
      <li>
        <a href="jaynes/articles/cdyson.pdf"><b>Disturbing the Memory</b></a
        >. Thoughts on Freeman Dyson's &quot;Disturbing the Universe&quot; 1979.
      </li>
      <li>
        <a href="jaynes/articles/cinfscat.pdf"><b>Inferential Scattering</b></a
        >. &quot;Generalized Scattering&quot; at the Second Maximum Entropy
        Workshop Laramie WY, 1982. in
        <i>Maximum-Entropy and Bayesian Methods in Inverse Problems</i>, C.R.
        Smith &amp; W.T. Grandy eds. (1985). Revised and extended here 1993. --
        propagation of inferences is just like Quantum scattering theory.
      </li>
      <li>
        <a href="jaynes/articles/cchirp.pdf"
          ><b>Bayesian Spectrum and Chirp Analysis</b></a
        >. Revised and updated paper presented at the Third Workshop on
        Maximum-Entropy and Bayesian Methods, Laramie Wyoming, Aug 1983.
        Published in
        <i>Maximum-Entropy and Spectral Analysis and Estimation Problems</i>,
        C.R. Smith and G.J. Erickson, Eds. 1987. Assumes spectrum of the form
        cos(wt + at^2 + theta). See the Bretthorst Lecture notes here (<a
          href="bretthorst/1.pdf"
          >1,</a
        ><a href="bretthorst/2.pdf">2</a>,<a href="bretthorst/3.pdf">3</a>,<a
          href="bretthorst/4.pdf"
          >4</a
        >).
      </li>
      <li>
        <a href="jaynes/articles/ccarnot.pdf"
          ><b>Evolution of Carnot's Principle</b></a
        >. presented at EMBO workshop on Maximum-Entropy in x-ray
        crystallography Orsay 1984.
      </li>
      <li>
        <a href="jaynes/articles/cmacro.pdf"><b>Macroscopic Prediction</b></a
        >. In Complex Systems-Operational Approaches in Neurobiology, Physics
        and Computers, H. Haken Ed. 1985.&quot;Maximum Caliber&quot; Principle.
        &quot;Why is it that knowledge of microphenomena does not seem
        sufficient to understand macrophenomena?..... A macrostate has a
        crucially important further property (entropy) that is not determined by
        the microstate&quot;.
      </li>
      <li>
        <a href="jaynes/articles/probtheoryaslogic.pdf"
          ><b>Probability Theory as Logic</b></a
        >. Presented at the 9th Annual Workshop on Maximum Entropy and Bayesian
        Methods, Dartmouth Colledge, New Hampshire, Aug 1989. In
        <i>Maximum Entropy and Bayesian Methods</i>, Paul F Fougere Ed. Kluwer
        (1990). Revised and extended 1994. The &quot;Mind Projection
        Fallacy&quot; and discusses AI.
      </li>

      <li>
        <a href="jaynes/articles/ceconent.pdf"
          ><b>How Should We Use Entropy in Economics?</b></a
        >. &quot;half baked ideas&quot;
      </li>

      <li>
        <a href="jaynes/articles/cplanets.pdf"
          ><b>Detection of Extra-Solar-System Planets</b></a
        >. In
        <i>Maximum-Entropy and Bayesian Methods in Science and Engineering</i>,
        Vol 1 G.J. Erickson &amp; C.R. Smith eds. Kluwer 1988. Using some of
        Bretthorst work. (Links given above)
      </li>
      <li>
        <a href="jaynes/articles/cmonkeys.pdf"
          ><b>Monkeys, Kangaroos, and N</b></a
        >. Presented at 4th Annual Workshop on Bayesian/Maximum Entropy Methods,
        Calgary 1984. In the Proceedings Volume,
        <i>Maximum Entropy and Bayesian Methods in Applied Statistics</i>, J
        Justice, Ed. Cam. Univ. Press 1986. Revised 1994. Good one for the
        effect of prior on the final probability.
      </li>
      <li>
        <a href="jaynes/articles/cmystery.pdf"
          ><b>Clearing up the Mysteries-- The Original Goal</b></a
        >. Opening talk at the 8th Int. MAXENT Workshop, St John's College
        Cambridge England 1988. In the Proceedings Volume,
        <i>Maximum Entropy and Bayesian Methods</i>, J Skilling Ed. Kluwer 1989.
        Very Good description of the introduction of time asymmetry into
        diffusion equations and a proof of Fick's law. Bell inequalities and
        EPR. Bernoulli's Urn and Inference backward in time. Muscle efficiency.
      </li>
      <li>
        <a href="jaynes/articles/cnova.pdf"
          ><b>Scattering of Light by Free Electrons</b></a
        >. Presented at the Workshop &quot;The Electron 1990&quot; at St.
        Francis Xavier University, Antogonosh, Nova Scotia, 1990. to be
        published in the proceedings vol, A. Weingartshofer &amp; D. Hestenes
        Eds. Jaynes tackling QM scattering theory.
      </li>
      <li>
        <a href="jaynes/articles/craddis.pdf"
          ><b>The Theory of Radar Target Discrimination</b></a
        >. 1989. 20 pages.
      </li>
      <li>
        <a href="jaynes/articles/csf89e.pdf"
          ><b>Probability in Quantum Theory</b></a
        >. Revised and extended version of a paper presented at the Workshop on
        Complexity, Entropy and the Physics of Information, Santa Fe, New Mexico
        1989. Original version in the Proceedings Volume,
        <i>Complexity, Entropy and the Physics of Information</i>, W.H. Zurek
        Ed. 1990. Good bit on the Lamb shift using field oscillators (page 13).
        Equation for friction etc. see Chandler
        <i>Intro to Modern Stat Mech</i>. Langevin equation page 260.
      </li>
      <li>
        <a href="jaynes/articles/cgibbs.pdf"><b>The Gibbs Paradox</b></a
        >. In <i>Maximum Entropy and Bayesian Methods</i>, C.R. Smith, G.J.
        Erickson, &amp; P.O. Neudorfer, Eds. Kluwer 1992. Good discussion of
        second law/Maxwell's demon using two kinds on argon and
        &quot;Whifnium&quot; showing how Entropy is information.
      </li>
      <li>
        <a href="jaynes/articles/lar92.pdf"
          ><b>A Backward look to the Future</b></a
        >. 1993 in
        <i>Physics and Probability: Essays in honor on Edwin T Jaynes</i>. W.T
        Grandy &amp; P.W. Milonni eds Cambridge University Press. Jaynes takes
        on his detractors.
      </li>
      <li>
        <a href="jaynes/articles/larami90.pdf"
          ><b>Notes on Present Status and Future Prospects</b></a
        >. (13 pages) AI, MYCIN and probabilities.
      </li>
      <li>
        <a href="jaynes/articles/los91.pdf"
          ><b>Commentary on Two Articles bu C.A. Los</b></a
        >. In Vol 3 of special issues. &quot;On System-theoretic Methods in
        Economic Modelling&quot;, S. Mittnik Ed. in<i>
          Computers and Mathematics with Applications</i
        >.
      </li>
      <li>
        <a href="jaynes/articles/sfb.pdf"
          ><b>The Second Law as Physical Fact and as Human Inference</b></a
        >. (1998?) What does reversibility mean? Second law means
        &quot;reproducibility&quot; not &quot;disorder&quot;.
      </li>
      <li>
        <a href="jaynes/articles/silver.pdf"
          ><b>Note on Thermal Heating Efficiency</b></a
        >. Shows how second law + heat pump can deliver more efficient heaters!
        Concise (5 pages) and interesting!
      </li>
      <li>
        <a href="jaynes/articles/ccalgtut.pdf"
          ><b>Bayesian Methods: General Background</b></a
        >. Presented at the Forth Annual Workshop on Bayesian/Maximum Entropy
        Methods, University of Calgary, August 1984. In the Proceddings Volumne,
        <i>Maximum Entropy and Bayesian Methods in Applied Statistics</i> J.H.
        Justice Editor, Cambridge University Press (1985) pp 1-25.
      </li>
      <li>
        <a href="jaynes/articles/val1.pdf"><b>Highly Informative Priors</b></a
        >. <i>Bayesian Statistics 2</i>. pp 329-360. Bernardo, DeGroot, Lindley,
        Smith Eds. Elsevier Science 1985. (31 pages) An historical survey,
        followed by a worked-out example (seasonal adjustment in econometrics)
        show how much prior information can affect our final conclusions in a
        way that cannot even be stated in the language of orthodox statistical
        theory, because it does not admit the concept of correlations in a
        posterior distribution function. Has a discussion at the end.
      </li>

      <li>
        <a href="jaynes/articles/cgap.pdf"
          ><b>Recolections and Mementos of G. A. Pfeiffer</b></a
        >.
      </li>

      <li>
        <a href="jaynes/articles/exchange.pdf"
          ><b
            >Some Applications and Extensions of the De Finetti Representation
            Theorem</b
          ></a
        >. To appear in
        <i
          >Bayesian Inference and Decision Techniques with Applications: Essays
          in Honor of Bruno de Finetti</i
        >, Prem K. Goel and Arnold Zellener, Editors, North-Holland Publishers,
        Amsterdam.
      </li>
      <li>
        <a href="jaynes/articles/rational.pdf"
          ><b>On The Rationale of Maximum-Entropy Methods</b></a
        >. Proceedings of the IEEE, Vol. 70, No. 9, September 1982.
      </li>
    </ol>
  </body>
</html>
